{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ZipFile.__del__ at 0x00000225F1EE5A60>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ilija\\AppData\\Local\\Programs\\Python\\Python38\\lib\\zipfile.py\", line 1821, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\ilija\\AppData\\Local\\Programs\\Python\\Python38\\lib\\zipfile.py\", line 1838, in close\n",
      "    self.fp.seek(self.start_dir)\n",
      "ValueError: seek of closed file\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "# import comet_ml\n",
    "# from comet_ml import Experiment\n",
    "# from comet_ml import OfflineExperiment\n",
    "import tensorflow as tf\n",
    "import importlib\n",
    "import module_imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "importlib.reload(module_imports)\n",
    "from module_imports import *\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-1-before\n",
      "Number of rows removed:  0\n",
      "1-0-before\n",
      "Number of rows removed:  0\n",
      "1-0-after\n",
      "Number of rows removed:  0\n",
      "5-1-after\n",
      "Number of rows removed:  0\n",
      "10-5-after\n",
      "Number of rows removed:  0\n",
      "15-10-after\n",
      "Number of rows removed:  0\n"
     ]
    }
   ],
   "source": [
    "INTERVAL_ID = 30\n",
    "SCRIPT_ID = f'INST_{INTERVAL_ID}M_SDANN_SDNN_SD2_ZScoreByFeature'\n",
    "\n",
    "# dirs\n",
    "results_dir = module_paths.Path(f'Results_{module_time.ymd()}'); results_dir.create_dir(warn_exists = False)\n",
    "cv_results_dir = module_paths.Path(f'CVResults_{module_time.ymd()}'); cv_results_dir.create_dir(warn_exists = False)\n",
    "exception_dir = module_paths.Path(f'Exceptions'); exception_dir.create_dir(warn_exists = False)\n",
    "metadata_dir = module_paths.Path('Metadata'); metadata_dir.create_dir(warn_exists = False)\n",
    "metadata_folds_dir = module_paths.Path(f'Metadata\\\\Folds'); metadata_folds_dir.create_dir(warn_exists = False)\n",
    "metadata_search_space_dir = module_paths.Path(f'Metadata\\\\SearchSpace'); metadata_search_space_dir.create_dir(warn_exists = False)\n",
    "metadata_models_dir = module_paths.Path(f'ModelsData\\\\Models'); metadata_models_dir.create_dir(warn_exists = False)\n",
    "trained_models_dir = module_paths.Path(f'TrainedModels'); trained_models_dir.create_dir()\n",
    "\n",
    "# files\n",
    "exceptions_file_path = f'{exception_dir.path}\\\\Exceptions_{SCRIPT_ID}.txt'\n",
    "\n",
    "it # DATASET\n",
    "HRV_FEATURES = ['SDNN','SDNN-1', 'ASDNN','ASDNN-1','ASDNN-2','ASDNN-3', 'SDANN','SDANN-1','SDANN-2','SDANN-3',\n",
    "                'NN50','NN50-1','pNN50','pNN50-1', 'rMSSD','rMSSD-1','SD1','SD2','SD1/SD2']\n",
    "\n",
    "WINDOWS_BEFORE = ['5-1', '1-0']\n",
    "WINDOWS_AFTER = [f'{x}-{x-5}' for x in [15,10]] + ['5-1', '1-0']; WINDOWS_AFTER.reverse()\n",
    "WINDOWS = [f'{w}-before' for w in WINDOWS_BEFORE] + [f'{w}-after' for w in WINDOWS_AFTER]\n",
    "\n",
    "\n",
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = module_preprocessing.remove_outliers(df, features_to_analyze = set(df.columns) - {'Class', 'Glucose', 'Patient_ID'}, by_feature = True, method = 'z-score')\n",
    "    df = module_preprocessing.power_transform(dataframe = df, features_to_ignore = {'Class', 'Patient_ID'})\n",
    "    return df[['Patient_ID', 'Class'] + HRV_FEATURES].reset_index(drop = True)\n",
    "\n",
    "\n",
    "dataset_all_windows = pd.read_excel(f'../data/Fasting_{INTERVAL_ID}min_instantaneous.xlsx')\n",
    "dataset_all_windows = dataset_all_windows[dataset_all_windows['Class'] != 'ND']\n",
    "dataset_all_windows.drop('Glucose', axis = 1, inplace = True)\n",
    "dataset_all_windows.replace({'GD': 0, 'BD': 1}, inplace = True)\n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "for window in WINDOWS:\n",
    "    print(window)\n",
    "\n",
    "    # select window, remove columns\n",
    "    dataset_window = dataset_all_windows[(dataset_all_windows['Window'] == '-'.join(window.split('-')[:-1])) & (dataset_all_windows['BeforeAfter'] == window.split('-')[-1])].copy()\n",
    "    dataset_window.drop(['Window', 'BeforeAfter'], axis = 1, inplace = True)\n",
    "\n",
    "    # inplace preprocessing\n",
    "    dataset_window = preprocess_dataset(dataset_window)\n",
    "    dataset_window['Window'] = window\n",
    "    dataset = pd.concat([dataset, dataset_window], sort = False).reset_index(drop = True)\n",
    "\n",
    "dataset.replace(\n",
    "        {\n",
    "            '5-1-before': -5,\n",
    "            '1-0-before': -1,\n",
    "            '1-0-after': 1,\n",
    "            '5-1-after': 5,\n",
    "            '10-5-after': 10,\n",
    "            '15-10-after': 15,\n",
    "        },\n",
    "        inplace = True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# FEATURE SELECTION\n",
    "dataset = dataset[['Patient_ID', 'Class', 'SDANN', 'SDANN-1', 'SDNN', 'SDNN-1', 'SD2', 'Window']]\n",
    "dataset.dropna(axis = 0, inplace = True)\n",
    "\n",
    "# SEARCH SPACE\n",
    "search_space = dict(\n",
    "        batch_size = [1, 5],\n",
    "        learning_rate = [0.2, 0.1, 0.02],\n",
    "        optimizer = ['Nadam', 'Adam', 'RMSProp'],\n",
    "        loss = ['CategoricalHinge', 'Poisson', 'CategoricalCrossentropy'],\n",
    "        layers_list = [\n",
    "            [\n",
    "                {'units': 6, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 3, 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "            [\n",
    "                {'units': 6, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 4, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 2, 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "            [\n",
    "                {'units': 6, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 5, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 4, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 3, 'type': 'dense', 'activation': 'relu'},\n",
    "\n",
    "            ],\n",
    "            [\n",
    "                {'units': 6, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 5, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 4, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 3, 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': 2, 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "        ],\n",
    "        regularizer = ['tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)', None],\n",
    "        initializer = ['VarianceScaling', 'GlorotNormal'],\n",
    "        patience = [100],\n",
    "        decay = [1e-2],\n",
    "        early_stopping_flag = [True],\n",
    "        validation_split = [0.05],\n",
    "        epochs = [50,100,1000],\n",
    ")\n",
    "\n",
    "# combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "combinations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# tabular combinations\n",
    "model_names = [f'M{i+1}' for i in range(len(combinations_dicts))]\n",
    "combinations_df = pd.DataFrame(combinations_dicts)\n",
    "combinations_df.insert(0, 'Model', model_names)\n",
    "combinations_df['Trained'] = 'No'\n",
    "\n",
    "# save\n",
    "search_space_path = f'{metadata_search_space_dir.path}\\\\search_space_{SCRIPT_ID}.xlsx'\n",
    "combinations_df.to_excel(search_space_path, index = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Overlap: []\n",
      "+----+--------------+------------------+-------------+-----------------+--------------+------------------+-------------+-----------------+----------------------+--------------------+------------+-----------+-----------+\n",
      "|    |   0 patients |   0 patients (%) |   0 samples |   0 samples (%) |   1 patients |   1 patients (%) |   1 samples |   1 samples (%) |   Balance of samples |   Balance patients |   Patients |   Samples | dataset   |\n",
      "|----+--------------+------------------+-------------+-----------------+--------------+------------------+-------------+-----------------+----------------------+--------------------+------------+-----------+-----------|\n",
      "|  0 |            1 |           0.3333 |          22 |          0.2558 |            2 |           0.6667 |          64 |          0.7442 |               2.9091 |                  2 |          3 |        86 | fold0     |\n",
      "+----+--------------+------------------+-------------+-----------------+--------------+------------------+-------------+-----------------+----------------------+--------------------+------------+-----------+-----------+\n",
      "Training model:\n",
      " Model                                                                 M1\n",
      "batch_size                                                             1\n",
      "learning_rate                                                        0.2\n",
      "optimizer                                                          Nadam\n",
      "loss                                                    CategoricalHinge\n",
      "layers_list            [{'units': 6, 'type': 'dense', 'activation': '...\n",
      "regularizer                tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
      "initializer                                              VarianceScaling\n",
      "patience                                                             100\n",
      "decay                                                               0.01\n",
      "early_stopping_flag                                                 True\n",
      "validation_split                                                    0.05\n",
      "epochs                                                                50\n",
      "Trained                                                               No\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilija\\Pycharm Projects\\GLYCO_open\\modules\\module_dl.py:568: UserWarning: Binary classifier ends with a layer with more than 2 units.Appending a sigmoid classification layer as a last layer of the neural network!\n",
      "  warnings.warn('Binary classifier ends with a layer with more than 2 units.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 14/50\n",
      "Epoch 15/50\n",
      "Epoch 16/50\n",
      "Epoch 17/50\n",
      "Epoch 18/50\n",
      "Epoch 19/50\n",
      "Epoch 20/50\n",
      "Epoch 21/50\n",
      "Epoch 22/50\n",
      "Epoch 23/50\n",
      "Epoch 24/50\n",
      "Epoch 25/50\n",
      "Epoch 26/50\n",
      "Epoch 27/50\n",
      "Epoch 28/50\n",
      "Epoch 29/50\n",
      "Epoch 30/50\n",
      "Epoch 31/50\n",
      "Epoch 32/50\n",
      "Epoch 33/50\n",
      "Epoch 34/50\n",
      "Epoch 35/50\n",
      "Epoch 36/50\n",
      "Epoch 37/50\n",
      "Epoch 38/50\n",
      "Epoch 39/50\n",
      "Epoch 40/50\n",
      "Epoch 41/50\n",
      "Epoch 42/50\n",
      "Epoch 43/50\n",
      "Epoch 44/50\n",
      "Epoch 45/50\n",
      "Epoch 46/50\n",
      "Epoch 47/50\n",
      "Epoch 48/50\n",
      "Epoch 49/50\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
      "C:\\Users\\ilija\\Pycharm Projects\\GLYCO_open\\modules\\module_dl.py:362: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "  npv = round(true_negatives / (true_negatives + false_negatives), 4)\n",
      "C:\\Users\\ilija\\AppData\\Local\\Temp/ipykernel_9260/1686714756.py:156: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  .append(pd.DataFrame(model_fold_results_list).mean())\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model:\n",
      " Model                                                                 M2\n",
      "batch_size                                                             1\n",
      "learning_rate                                                        0.2\n",
      "optimizer                                                          Nadam\n",
      "loss                                                    CategoricalHinge\n",
      "layers_list            [{'units': 6, 'type': 'dense', 'activation': '...\n",
      "regularizer                tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
      "initializer                                              VarianceScaling\n",
      "patience                                                             100\n",
      "decay                                                               0.01\n",
      "early_stopping_flag                                                 True\n",
      "validation_split                                                    0.05\n",
      "epochs                                                               100\n",
      "Trained                                                               No\n",
      "Name: 1, dtype: object\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 21        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 4         \n",
      "=================================================================\n",
      "Total params: 67\n",
      "Trainable params: 67\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "Epoch 2/100\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_9260/1686714756.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    135\u001B[0m                                                                 \u001B[0mmodel_id\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel_id\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    136\u001B[0m                                                                 )\n\u001B[1;32m--> 137\u001B[1;33m             \u001B[0mbinary_classifier\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    138\u001B[0m             \u001B[0mtest_report\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbinary_classifier\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreturn_metrics\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Pycharm Projects\\GLYCO_open\\modules\\module_dl.py\u001B[0m in \u001B[0;36mfit_model\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    167\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    168\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 169\u001B[1;33m             fit_metadata_tracker = self.model.fit(x = self.split_sets['X_train'],\n\u001B[0m\u001B[0;32m    170\u001B[0m                                                   \u001B[0my\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msplit_sets\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'y_train'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    171\u001B[0m                                                   \u001B[0mepochs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhyper_params\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'epochs'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\comet_ml\\monkey_patching.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    315\u001B[0m                     )\n\u001B[0;32m    316\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 317\u001B[1;33m         \u001B[0mreturn_value\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0moriginal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    318\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    319\u001B[0m         \u001B[1;31m# Call after callbacks once we have the return value\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1098\u001B[0m                 _r=1):\n\u001B[0;32m   1099\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1100\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1101\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1102\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    826\u001B[0m     \u001B[0mtracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    827\u001B[0m     \u001B[1;32mwith\u001B[0m \u001B[0mtrace\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTrace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_name\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtm\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 828\u001B[1;33m       \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    829\u001B[0m       \u001B[0mcompiler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"xla\"\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_experimental_compile\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;34m\"nonXla\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    830\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    853\u001B[0m       \u001B[1;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    854\u001B[0m       \u001B[1;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 855\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=not-callable\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    856\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    857\u001B[0m       \u001B[1;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2940\u001B[0m       (graph_function,\n\u001B[0;32m   2941\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m-> 2942\u001B[1;33m     return graph_function._call_flat(\n\u001B[0m\u001B[0;32m   2943\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0;32m   2944\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1916\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1917\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1918\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1919\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1920\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    553\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    554\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 555\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    556\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    557\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\ilija\\pycharm projects\\glyco_open\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# FOLDS\n",
    "\n",
    "random_states = [11,22,33,44,55,66,77,88,99,111]\n",
    "\n",
    "split_sets_cv = dict()\n",
    "train_folds_analysis_dict = dict()\n",
    "test_folds_analysis_dict = dict()\n",
    "train_folds_analysis_dict_overleaf = dict()\n",
    "test_folds_analysis_dict_overleaf = dict()\n",
    "\n",
    "for fold_id in range(1):\n",
    "\n",
    "    fold_exists = True\n",
    "    while fold_exists:\n",
    "\n",
    "        train, test = module_train_test.find_best_group_split(dataframe = dataset,\n",
    "                                                             target_feature = 'Class',\n",
    "                                                             group_by_feature = 'Patient_ID',\n",
    "                                                             balance_focus = 'train_with_test_threshold',\n",
    "                                                             test_balance_treshold = 3,\n",
    "                                                             num_splits_to_try = 1000,\n",
    "                                                             random_state = random_states[fold_id],\n",
    "                                                             test_size = 0.1)\n",
    "\n",
    "        train_folds_analysis_dict[fold_id] = module_dataset_analysis.quantitative_analysis(df = train, dataset_name = f'fold{fold_id}',\n",
    "                                                                                          class_feature = 'Class', classes = [0, 1])\n",
    "        test_folds_analysis_dict[fold_id] = module_dataset_analysis.quantitative_analysis(df = test, dataset_name = f'fold{fold_id}',\n",
    "                                                                                         class_feature = 'Class', classes = [0, 1])\n",
    "\n",
    "        train_folds_analysis_dict_overleaf[fold_id] = module_dataset_analysis.quantitative_analysis(df = train, dataset_name = f'fold{fold_id}',\n",
    "                                                                                                   class_feature = 'Class', classes = [0, 1],\n",
    "                                                                                                   overleaf = True)\n",
    "        test_folds_analysis_dict_overleaf[fold_id] = module_dataset_analysis.quantitative_analysis(df = test, dataset_name = f'fold{fold_id}',\n",
    "                                                                                                  class_feature = 'Class', classes = [0, 1],\n",
    "                                                                                                  overleaf = True)\n",
    "\n",
    "        print(fold_id)\n",
    "        if fold_id == 0:\n",
    "            fold_exists = False\n",
    "        else:\n",
    "            for prev_fold_id in range(fold_id):\n",
    "                print('Comparing with: \\t\\t\\t\\t',prev_fold_id)\n",
    "                for test_analysis_key in test_folds_analysis_dict[fold_id].keys():\n",
    "                    print('comparing', test_analysis_key)\n",
    "                    if type(test_folds_analysis_dict[fold_id][test_analysis_key]) is str:\n",
    "                        continue\n",
    "                    if round(test_folds_analysis_dict[fold_id][test_analysis_key], 2) != round(test_folds_analysis_dict[prev_fold_id][test_analysis_key], 2):\n",
    "                        print(f'{round(test_folds_analysis_dict[fold_id][test_analysis_key], 2)} != {round(test_folds_analysis_dict[prev_fold_id][test_analysis_key], 2)}')\n",
    "                        fold_exists = False\n",
    "                    else:\n",
    "                        print('EQUAL VALUES')\n",
    "\n",
    "        if fold_exists:\n",
    "            random_states[fold_id] += 1\n",
    "            print('_______________________________REPEATING SPLIT WITH DIFFERENT RANDOM SPLIT')\n",
    "\n",
    "    train.reset_index(inplace = True, drop = True)\n",
    "    test.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    train_patients = train.pop('Patient_ID')\n",
    "    test_patients = test.pop('Patient_ID')\n",
    "\n",
    "    overlap = list(set(train_patients.unique()).intersection(set(test_patients.unique())))\n",
    "    print('Overlap: ' + str(overlap))\n",
    "    assert dataset['Patient_ID'].nunique() == train_patients.nunique() + test_patients.nunique(), 'Problem wi`th patients'\n",
    "\n",
    "    split_sets_cv[f'y_train_{fold_id}'] = train.pop('Class').to_numpy()\n",
    "    split_sets_cv[f'y_test_{fold_id}'] = test.pop('Class').to_numpy()\n",
    "\n",
    "    standard_scaler = MinMaxScaler()\n",
    "\n",
    "    train = standard_scaler.fit_transform(train)\n",
    "    test = standard_scaler.transform(test)\n",
    "\n",
    "    split_sets_cv[f'X_train_{fold_id}'] = train\n",
    "    split_sets_cv[f'X_test_{fold_id}'] = test\n",
    "\n",
    "module_pandas_utils.print_df(pd.DataFrame(test_folds_analysis_dict).T)\n",
    "\n",
    "pd.Series(random_states, name = 'random_states').to_excel(f'{metadata_folds_dir.path}\\\\random_states_{SCRIPT_ID}.xlsx', index = False)\n",
    "\n",
    "pd.DataFrame(train_folds_analysis_dict).T.to_excel(f'{metadata_folds_dir.path}\\\\train_folds_analysis_{SCRIPT_ID}.xlsx', index = False)\n",
    "pd.DataFrame(test_folds_analysis_dict).T.to_excel(f'{metadata_folds_dir.path}\\\\test_folds_analysis_{SCRIPT_ID}.xlsx', index = False)\n",
    "pd.DataFrame(train_folds_analysis_dict_overleaf).T.to_excel(f'{metadata_folds_dir.path}\\\\train_folds_analysis_{SCRIPT_ID}_overleaf.xlsx', index = False)\n",
    "pd.DataFrame(test_folds_analysis_dict_overleaf).T.to_excel(f'{metadata_folds_dir.path}\\\\test_folds_analysis_{SCRIPT_ID}_overleaf.xlsx', index = False)\n",
    "\n",
    "# TRAINING\n",
    "\n",
    "start_time = module_time.hmymd()\n",
    "more_models_left_to_train = True\n",
    "\n",
    "fold_results_list = list()\n",
    "cv_results_list = list()\n",
    "\n",
    "while more_models_left_to_train:\n",
    "\n",
    "    search_space_state = pd.read_excel(search_space_path)\n",
    "\n",
    "    model_to_train = search_space_state.loc[search_space_state['Trained'] == 'No'].iloc[0]\n",
    "    print(f'Training model:\\n {model_to_train}')\n",
    "\n",
    "    search_space_state.loc[search_space_state['Model'] == model_to_train['Model'], 'Trained'] = 'InProgress'\n",
    "    search_space_state.to_excel(search_space_path, index = False)\n",
    "\n",
    "    model_fold_results_list = list()\n",
    "    for fold_id in range(1):\n",
    "\n",
    "        y_train = split_sets_cv[f'y_train_{fold_id}']\n",
    "        y_test = split_sets_cv[f'y_test_{fold_id}']\n",
    "        X_train = split_sets_cv[f'X_train_{fold_id}']\n",
    "        X_test = split_sets_cv[f'X_test_{fold_id}']\n",
    "\n",
    "        try:\n",
    "\n",
    "            model_id = f\"{model_to_train['Model']}_Fold{fold_id}\"\n",
    "\n",
    "            binary_classifier = module_dl.BinaryClassificationDL(X_train = X_train,\n",
    "                                                                y_train = y_train,\n",
    "                                                                X_test = X_test,\n",
    "                                                                y_test = y_test,\n",
    "                                                                epochs = model_to_train['epochs'],\n",
    "                                                                batch_size = model_to_train['batch_size'],\n",
    "                                                                learning_rate = model_to_train['learning_rate'],\n",
    "                                                                decay = model_to_train['decay'],\n",
    "                                                                validation_split = model_to_train['validation_split'],\n",
    "                                                                loss_name = model_to_train['loss'],\n",
    "                                                                initializer_name = model_to_train['initializer'],\n",
    "                                                                optimizer_name = model_to_train['optimizer'],\n",
    "                                                                early_stopping_flag = True,\n",
    "                                                                patience = model_to_train['patience'],\n",
    "                                                                layers_list = eval(model_to_train['layers_list']),\n",
    "                                                                model_type = 'sequential',\n",
    "                                                                regularizer = eval(model_to_train['regularizer']),\n",
    "                                                                trained_models_dir = trained_models_dir,\n",
    "                                                                model_id = model_id\n",
    "                                                                )\n",
    "            binary_classifier.fit_model()\n",
    "            test_report = binary_classifier.predict(return_metrics = True)\n",
    "\n",
    "            fold_results_list.append(dict(\n",
    "                model = f\"{model_to_train['Model']}_{fold_id}\",\n",
    "                **test_report,\n",
    "                model_hyper = f\"{model_to_train['Model']}_{fold_id}\",\n",
    "                **{key: value for (key, value) in model_to_train.to_dict().items() if key in search_space.keys() and len(search_space[key]) > 1},\n",
    "                **{key: value for (key, value) in model_to_train.to_dict().items() if key in search_space.keys() and len(search_space[key]) == 1},\n",
    "            ))\n",
    "            model_fold_results_list.append(test_report)\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(\"ERROR: \",str(e))\n",
    "            with open(exceptions_file_path, 'a') as exceptions_file:\n",
    "                exceptions_file.write(f'{str(e)}\\n')\n",
    "\n",
    "    model_cv_results = pd.Series({'Model': model_to_train['Model']})\\\n",
    "        .append(pd.DataFrame(model_fold_results_list).mean())\\\n",
    "        .append(model_to_train).to_dict()\n",
    "\n",
    "    cv_results_list.append(model_cv_results)\n",
    "\n",
    "    pd.DataFrame(fold_results_list).to_excel(f'{results_dir.path}\\\\Fold_Results_{SCRIPT_ID}.xlsx')\n",
    "    pd.DataFrame(cv_results_list).to_excel(f'{cv_results_dir.path}\\\\CV_Results_{SCRIPT_ID}.xlsx')\n",
    "\n",
    "    search_space_state = pd.read_excel(search_space_path)\n",
    "    search_space_state.loc[search_space_state['Model'] == model_to_train['Model'], 'Trained'] = 'Yes'\n",
    "    search_space_state.to_excel(search_space_path, index = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}